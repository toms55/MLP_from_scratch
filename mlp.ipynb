{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron (MLP) Implementation with C-Wrapper\n",
    "\n",
    "This notebook contains the implementation of an **MLP** class designed to interface with a custom C-wrapper (`c_wrapper`) for efficient, low-level matrix operations. It handles core neural network functions, including initialization, forward propagation, backpropagation, and training.\n",
    "\n",
    "The internal structure assumes samples are represented as columns (features $\\times$ samples), necessitating transpositions when interacting with external NumPy data (samples $\\times$ features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import c_wrapper\n",
    "import numpy as np\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:    \n",
    "    def __init__(self, layer_sizes: List[int], hidden_activation: str, output_activation: str, loss: str = \"MSE\", learning_rate=0.01, seed: Optional[int] = None):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.hidden_activation = hidden_activation.lower()\n",
    "        self.output_activation = output_activation.lower()\n",
    "        self.loss = loss\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activations = []\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            limit = np.sqrt(2 / layer_sizes[i])\n",
    "            w_np = np.random.randn(layer_sizes[i + 1], layer_sizes[i]) * limit\n",
    "            b_np = np.zeros((layer_sizes[i + 1], 1))\n",
    "\n",
    "            self.weights.append(c_wrapper.from_numpy(w_np))    \n",
    "            self.biases.append(c_wrapper.from_numpy(b_np))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `_clear_activations`: Memory Management\n",
    "\n",
    "This is a crucial helper function responsible for **explicit memory cleanup**. It iterates through the stored layer activations and calls `c_wrapper.free_py_matrix` to release the associated C-level memory after the backward pass, preventing memory leaks during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clear_activations(self):\n",
    "    for i in range(1, len(self.activations)):\n",
    "        c_wrapper.free_py_matrix(self.activations[i])\n",
    "    self.activations = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `forward_pass`: Feedforward Calculation\n",
    "\n",
    "This method computes the network's output. For each layer, it performs the linear transformation $Z = W \\cdot A_{\\text{prev}} + B$ and applies the specified **activation function** (`sigmoid`, `relu`, or `identity`) to get $A = g(Z)$. This involves matrix multiplication of the current weights with the previous layer's output, followed by adding the bias vector and running the result through the non-linear function. All intermediate activations $A$ are stored in self.activations for use during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(self, X: c_wrapper.Matrix):\n",
    "    self._clear_activations()\n",
    "\n",
    "    self.activations = [X]\n",
    "    cur_output = X\n",
    "    num_layers = len(self.weights)\n",
    "\n",
    "    for layer_index in range(num_layers):\n",
    "        weights = self.weights[layer_index]\n",
    "        biases = self.biases[layer_index]\n",
    "\n",
    "        weights_matrix = c_wrapper.multiply_py_matrices(\n",
    "            weights, cur_output)\n",
    "        input_matrix = c_wrapper.py_add_weights_and_biases(\n",
    "            weights_matrix, biases)\n",
    "\n",
    "        c_wrapper.free_py_matrix(weights_matrix)\n",
    "\n",
    "        is_output_layer = (layer_index == num_layers - 1)\n",
    "        activation_type = self.output_activation if is_output_layer else self.hidden_activation\n",
    "\n",
    "        if activation_type == \"sigmoid\":\n",
    "            activated_matrix = c_wrapper.py_sigmoid(input_matrix)\n",
    "            c_wrapper.free_py_matrix(input_matrix)\n",
    "        elif activation_type == \"relu\":\n",
    "            activated_matrix = c_wrapper.py_matrix_relu(input_matrix)\n",
    "            c_wrapper.free_py_matrix(input_matrix)\n",
    "        elif activation_type == \"identity\":\n",
    "            activated_matrix = input_matrix\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"The activation function {activation_type} has not been implemented\"\n",
    "            )\n",
    "\n",
    "        cur_output = activated_matrix\n",
    "        self.activations.append(cur_output)\n",
    "\n",
    "    return cur_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `backward_pass`: Backpropagation and Weight Update\n",
    "\n",
    "This method implements the **backpropagation algorithm**, which updates the network’s weights and biases by propagating errors backward through the layers. It begins by computing the gradient of the Mean Squared Error (MSE) loss and then applies the chain rule layer by layer.\n",
    "\n",
    "### Loss Gradient\n",
    "For MSE loss:\n",
    "$$\n",
    "\\nabla_{\\text{Loss}} = \\hat{y} - y\n",
    "$$\n",
    "\n",
    "### Output Layer\n",
    "The error signal (delta) at the output layer is computed as:\n",
    "$$\n",
    "\\delta_L = \\nabla_{\\text{Loss}} \\odot g'(Z_L)\n",
    "$$\n",
    "where $g'(Z_L)$ is the derivative of the output activation function.\n",
    "\n",
    "### Hidden Layers\n",
    "For hidden layers, the error signal propagates backward using:\n",
    "$$\n",
    "\\delta_l = (W_{l+1}^T \\cdot \\delta_{l+1}) \\odot g'(Z_l)\n",
    "$$\n",
    "This multiplies the next layer’s delta by the transposed weight matrix and then element-wise by the derivative of the current layer’s activation.\n",
    "\n",
    "### Gradient Computation\n",
    "Once all deltas are known, the gradients for the weights and biases are computed:\n",
    "$$\n",
    "\\nabla W_l = \\delta_l \\cdot A_{l-1}^T, \\quad\n",
    "\\nabla B_l = \\delta_l\n",
    "$$\n",
    "where $A_{l-1}$ is the activation from the previous layer.\n",
    "\n",
    "### Parameter Update\n",
    "Weights and biases are then updated using gradient descent:\n",
    "$$\n",
    "W_l = W_l - \\eta \\nabla W_l, \\quad\n",
    "B_l = B_l - \\eta \\nabla B_l\n",
    "$$\n",
    "Here, $\\eta$ is the learning rate controlling the step size, balancing convergence speed and stability.\n",
    "\n",
    "### Memory Management\n",
    "Throughout the backpropagation process, numerous temporary matrices are created to store deltas, transposes, gradients, and activation derivatives. The implementation explicitly frees these matrices as soon as they are no longer needed via `free_py_matrix`, ensuring efficient memory use and preventing leaks in environments without automatic garbage collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(self, y_true: c_wrapper.Matrix,\n",
    "                    y_pred: c_wrapper.Matrix):\n",
    "    if self.loss != \"MSE\":\n",
    "        raise ValueError(f\"{self.loss} has not been defined yet.\")\n",
    "\n",
    "    # Gradient for MSE: 2 * (y_pred - y_true) / N\n",
    "    diff = c_wrapper.subtract_py_matrices(y_pred, y_true)\n",
    "    output_error = c_wrapper.scalar_multiply_py_matrix(\n",
    "        diff, 2 / (y_true.cols * y_true.rows))\n",
    "    c_wrapper.free_py_matrix(diff)\n",
    "\n",
    "    num_layers = len(self.weights)\n",
    "\n",
    "    for layer_index in range(num_layers - 1, -1, -1):\n",
    "\n",
    "        is_output_layer = (layer_index == num_layers - 1)\n",
    "        activation_type = self.output_activation if is_output_layer else self.hidden_activation\n",
    "\n",
    "        activation_derivative = None\n",
    "        if activation_type == \"sigmoid\":\n",
    "            activation_derivative = c_wrapper.py_sigmoid_derivative(\n",
    "                self.activations[layer_index + 1])\n",
    "        elif activation_type == \"relu\":\n",
    "            activation_derivative = c_wrapper.py_matrix_relu_derivative(\n",
    "                self.activations[layer_index + 1])\n",
    "        elif activation_type == \"identity\":\n",
    "            # For identity, derivative is 1. We handle the error signal below.\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported activation {activation_type} in backward_pass\"\n",
    "            )\n",
    "\n",
    "        # Compute error signal (delta)\n",
    "        if layer_index == num_layers - 1:\n",
    "            # Output Layer: Loss_grad * Act_Derivative\n",
    "            if activation_type == \"identity\":\n",
    "                error_signal = c_wrapper.scalar_multiply_py_matrix(\n",
    "                    output_error, 1)\n",
    "            else:\n",
    "                error_signal = c_wrapper.hadamard_py_matrices(\n",
    "                    output_error, activation_derivative)\n",
    "        else:\n",
    "            # Hidden Layer: (W_T * Previous_Error) * Act_Derivative\n",
    "            transposed_weights = c_wrapper.transpose_py_matrix(\n",
    "                self.weights[layer_index + 1])\n",
    "            backpropagate = c_wrapper.multiply_py_matrices(\n",
    "                transposed_weights, output_error)\n",
    "            c_wrapper.free_py_matrix(transposed_weights)\n",
    "\n",
    "            if activation_type == \"identity\":\n",
    "                error_signal = backpropagate\n",
    "            else:\n",
    "                error_signal = c_wrapper.hadamard_py_matrices(\n",
    "                    backpropagate, activation_derivative)\n",
    "\n",
    "            c_wrapper.free_py_matrix(backpropagate)\n",
    "\n",
    "        # Weight Gradient = Error_Signal * A_{L-1}^T\n",
    "        transposed_prev = c_wrapper.transpose_py_matrix(\n",
    "            self.activations[layer_index])\n",
    "\n",
    "        # Calculate gradients\n",
    "        weight_grad_temp = c_wrapper.multiply_py_matrices(\n",
    "            error_signal, transposed_prev)\n",
    "        weight_grad = c_wrapper.scalar_multiply_py_matrix(\n",
    "            weight_grad_temp, self.learning_rate)\n",
    "\n",
    "        bias_grad_temp = c_wrapper.sum_py_matrix_columns(error_signal)\n",
    "        bias_grad = c_wrapper.scalar_multiply_py_matrix(\n",
    "            bias_grad_temp, self.learning_rate)\n",
    "\n",
    "        # Clean up temporary gradient matrices\n",
    "        c_wrapper.free_py_matrix(weight_grad_temp)\n",
    "        c_wrapper.free_py_matrix(bias_grad_temp)\n",
    "\n",
    "        # W_new = W_old - Grad\n",
    "        new_weights = c_wrapper.subtract_py_matrices(\n",
    "            self.weights[layer_index], weight_grad)\n",
    "        new_biases = c_wrapper.subtract_py_matrices(\n",
    "            self.biases[layer_index], bias_grad)\n",
    "\n",
    "        # Free the old weights/biases pointers\n",
    "        c_wrapper.free_py_matrix(self.weights[layer_index])\n",
    "        c_wrapper.free_py_matrix(self.biases[layer_index])\n",
    "\n",
    "        # Update to new pointers\n",
    "        self.weights[layer_index] = new_weights\n",
    "        self.biases[layer_index] = new_biases\n",
    "\n",
    "        # Clean up temporary matrices (derivatives, transposes, gradients)\n",
    "        if activation_derivative is not None:\n",
    "            c_wrapper.free_py_matrix(activation_derivative)\n",
    "        c_wrapper.free_py_matrix(transposed_prev)\n",
    "        c_wrapper.free_py_matrix(weight_grad)\n",
    "        c_wrapper.free_py_matrix(bias_grad)\n",
    "\n",
    "        # Free the error matrix from the previous layer\n",
    "        c_wrapper.free_py_matrix(output_error)\n",
    "        output_error = error_signal\n",
    "\n",
    "    c_wrapper.free_py_matrix(output_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `train_model`: Training Loop,\n",
    "\n",
    "This method executes the main training loop. It first converts and transposes the NumPy input data (`X`, `y`) into the C-wrapper format. For the specified number of epochs, it performs a full forward pass to get predictions, calculates the loss, and executes the backward pass to update the model parameters. The input matrices are freed after training completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(self, X: np.ndarray, y: np.ndarray, epochs: int):\n",
    "    X_c = c_wrapper.transpose_py_matrix(c_wrapper.from_numpy(X))\n",
    "    y_c = c_wrapper.transpose_py_matrix(c_wrapper.from_numpy(y))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Training epoch {epoch}\\n\")\n",
    "\n",
    "        y_pred = self.forward_pass(X_c)\n",
    "\n",
    "        loss = c_wrapper.py_mean_squared_error(y_c, y_pred)\n",
    "        print(f\"This epoch's loss is {loss:.6f}\")\n",
    "\n",
    "        self.backward_pass(y_c, y_pred)\n",
    "\n",
    "        self._clear_activations()\n",
    "\n",
    "    c_wrapper.free_py_matrix(X_c)\n",
    "    c_wrapper.free_py_matrix(y_c)\n",
    "\n",
    "    print(\"Training Complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `predict`: Inference\n",
    "\n",
    "Used for inference on new data. It performs a single **forward pass** on the input data, and importantly, **transposes the final output back** from the internal (features $\\times$ samples) format to the external NumPy standard (samples $\\times$ features) before returning the results to the user. All temporary C-matrices and activations are freed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X: np.ndarray):\n",
    "    X_c = c_wrapper.transpose_py_matrix(c_wrapper.from_numpy(X))\n",
    "\n",
    "    y_pred = self.forward_pass(X_c)\n",
    "    # Transpose the output back to NumPy's (samples, features) format\n",
    "    y_pred_transposed = c_wrapper.transpose_py_matrix(y_pred)\n",
    "\n",
    "    prediction = c_wrapper.to_numpy(y_pred_transposed)\n",
    "\n",
    "    c_wrapper.free_py_matrix(X_c)\n",
    "    c_wrapper.free_py_matrix(y_pred_transposed)\n",
    "    self._clear_activations()\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eef07e",
   "metadata": {},
   "source": [
    "To run and evaluate the model, run the below code. A lot of the code to evaluate the model was written by gemini. The reference to the chat is in the appendix. The code is not in this notebook, but a separate library to highlight it's conceptual shift from a theoretical model to a practical implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e36f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 python/run.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
